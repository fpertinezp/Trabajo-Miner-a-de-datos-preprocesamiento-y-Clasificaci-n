---
title: "Pipeline Versión 0"
author: "Francisco Pertíñez Perea"
lang: es
format:
  html:
    code-tools: true
    code-fold: true
---

**Características**:

- Eliminación X4
- Imputación valores faltantes X21 --> SimpleImputer()
- Tratamiento de valores atípicos con -> IsolationForest() 


```{python}
import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
```

```{python}
train_data = pd.read_csv("../data/training_data.csv", sep = ',', header = 0, 
                         na_values = ['?','','NA'])
test_data = pd.read_csv("../data/test_data.csv", sep = ',', header = 0, 
                        na_values = ['?','','NA'])
```

```{python}
train_ids = train_data["ID"]
test_ids = test_data["ID"]
train_target = train_data['RATE']
train_data = train_data.drop("RATE", axis = 1)
```

```{python}
train_data['Dataset'] = 'Train'
test_data['Dataset'] = 'Test'
data = pd.concat([train_data, test_data], ignore_index=True)
```

```{python}
data = data.drop('ID', axis = 1)
data = data.drop('X4', axis = 1)
```

```{python}
encoder = OrdinalEncoder(categories=[['VLOW', 'LOW', 'MED', 'HIGH', 'VHIGH']], 
                         dtype=int)
data["X24"] = encoder.fit_transform(data[["X24"]])
```

```{python}
encoder = OneHotEncoder(drop='if_binary')
df_one_hot = pd.DataFrame(encoder.fit_transform(data[['X25']]).toarray(), 
                          columns=encoder.get_feature_names_out(['X25']))
data = pd.concat([data, df_one_hot], axis=1)
data = data.drop('X25', axis=1)
```

```{python}
encoder = OneHotEncoder(sparse_output=False, dtype=np.float64)
df_one_hot = pd.DataFrame(encoder.fit_transform(data[['X30']]), 
                          columns=encoder.get_feature_names_out(['X30']))
data = pd.concat([data, df_one_hot], axis=1)
data = data.drop('X30', axis=1)
```

```{python}
clf = IsolationForest()
clf.fit(data[data['Dataset'] == 'Train'].drop(['Dataset', 'X21'], axis=1))
outliers = clf.predict(data[data['Dataset'] == 'Train'].drop(['Dataset', 'X21'], axis=1)) < 1
```

```{python}
data[data['Dataset'] == 'Train'] = data[data['Dataset'] == 'Train'][~outliers]
train_target = train_target[~outliers]
```

```{python}
preprocessor = ColumnTransformer(
    transformers=[
        ('impute_X21', SimpleImputer(), ['X21'])
    ],

    remainder='passthrough'
)

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('scaler', StandardScaler()),
    ('lgr', LogisticRegression())
])
```


```{python}
grid_params = {
    'preprocessor__impute_X21__strategy': ['mean', 'median', 'most_frequent'],
    'lgr__penalty': ['l1', 'l2'],
    'lgr__C': np.logspace(-3, 3, 50),
    'lgr__solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']
}
```


```{python}
grid_search = GridSearchCV(pipeline, grid_params, cv=5, scoring='accuracy', verbose=1)
```


```{python}
X_train = data[data['Dataset'] == 'Train'].drop('Dataset', axis=1)
y_train = train_target
X_test = data[data['Dataset'] == 'Test'].drop('Dataset', axis=1)

grid_search.fit(X_train, y_train)
```

```{python}
grid_search.best_score_
```


```{python}
grid_search.best_params_
```

```{python}
best_preprocessor = ColumnTransformer(
    transformers=[
        ('impute_X21', SimpleImputer(strategy=grid_search.best_params_['preprocessor__impute_X21__strategy']), ['X21'])
    ],

    remainder='passthrough'
)

best_model = LogisticRegression(C=grid_search.best_params_['lgr__C'],
                                penalty=grid_search.best_params_['lgr__penalty'],
                                solver=grid_search.best_params_['lgr__solver'])

pipeline = Pipeline([
    ('preprocessor', best_preprocessor),
    ('scaler', StandardScaler()),
    ('lgr', best_model)
])

pipeline.fit(X_train,y_train)
y_pred = pipeline.predict(X_test)
```


```{python}
y_pred
```

```{python}
sum_tr = 0
sum_te = 0
N = 100
for i in range(N):
    X_tr, X_te, y_tr, y_te = train_test_split(X_train, y_train, 
                                            test_size=0.3)
    pipeline.fit(X_tr, y_tr)

    y_pred_tr = pipeline.predict(X_tr)
    y_pred_te = pipeline.predict(X_te)

    sum_tr += sum(y_pred_tr == y_tr)/y_tr.shape[0]
    sum_te += sum(y_pred_te == y_te)/y_te.shape[0]

print('Train accuracy:', sum_tr/N)
print('Test accuracy:', sum_te/N)
```

```{python}
#result = pd.DataFrame({'ID': test_ids, 'RATE': y_pred})
#result.to_csv('../prediction/predV5.csv', index=False)
```

```{python}
directory = '../prediction/'
files = os.listdir(directory)

same = 0
different = 0

for file in files:
    df_csv = pd.read_csv(directory + file)
    different_indices = np.where(df_csv['RATE'] != y_pred)[0]
    
    if len(different_indices) == 0:
        same += 1
        print(f'Same file: {file}')
        print('\n')
    else:
        different += 1
        print(f'Different file: {file}')
        print(f'Number of differences: {len(different_indices)}')
        print('\n')

print('\n\n')
print(f'Same: {same - 1}')  # Minus itself
print(f'Different: {different}')
```

# Resultados

{'lgr__C': 0.655128556859551,
 'lgr__penalty': 'l1',
 'lgr__solver': 'saga',
 'preprocessor__impute_X21__strategy': 'most_frequent'}

Entrenamiento -> 0.6751646726710578

---

{'lgr__C': 0.21209508879201905,
 'lgr__penalty': 'l1',
 'lgr__solver': 'saga',
 'preprocessor__impute_X21__strategy': 'most_frequent'}

 Entrenamiento -> 0.6666394668842649
